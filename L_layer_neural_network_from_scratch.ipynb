{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L-layer neural network from scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmrj+TdBKnVzZCX/Bpnrnu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjT6PYLUReZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GElPj4W3CoKu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Initialize the parameters\n",
        "2.   Forward propagation\n",
        "3.   Compute the loss\n",
        "4.   Backward propagation\n",
        "5.   Updating the parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMrUxZWTDCqh",
        "colab_type": "text"
      },
      "source": [
        "# 1.Initialization of the parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_fpxDI_GJdf",
        "colab_type": "text"
      },
      "source": [
        "Here values of parameters will be randomly assigned. Hovewer we need to determine the shapes of the parameters.To do so we can do the following analysis. $$Z^{[i]}=W^{[i]}.a^{[i-1]}+b^{[i]}$$ Where $a^{[1]}=x_{[1]}.$  and $a^{[i]}=\\sigma(z^{[i]}).$ or $a^{[i]}=RELU(z^{[i]}).$ \\\n",
        "Dimensions of $Z^{[i]}$ is $(n^{[i]},1)$. So dimensions of $W^{[i]}$ is $(n^{[i]},n^{[i-1]})$ and dimensions of $b^{[i]}$ is $(n^{[i]},1).$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzOJQzsGDVUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    #layer dims will be a list which contains dimension in each layer where layer_dims[0] denoting number of input parameters.\n",
        "    parameters={} #dictionary containing parameters\n",
        "    for i in range(1,len(layer_dims)):\n",
        "        parameters[\"W\"+str(i)]=np.random.randn(layer_dims[i],layer_dims[i-1])\n",
        "        parameters[\"b\"+str(i)]=np.zeros((layer_dims[i],1))\n",
        "    assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "    assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRL-sn-zIyGe",
        "colab_type": "text"
      },
      "source": [
        "# 2. Forward Propagation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM8g9AkwLli4",
        "colab_type": "text"
      },
      "source": [
        "Now that $W^{[i]}$ and $b^{[i]}$ values are assigned we can find the input of the activation function.\n",
        "$$Z^{[i]}=W^{[i]}.a^{[i-1]}+b^{[i]}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYOu6VYhI8yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_Z(a,W,b):\n",
        "    Z=np.dot(W,a)+b\n",
        "    cache=(a,W,b)  #will be useful for backward propagation\n",
        "    return Z,cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0eioQgFOCUU",
        "colab_type": "text"
      },
      "source": [
        "In this neural network i will use ReLu activation function and sigmoid activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXikgaXb33cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#relu and sigmoid function\n",
        "def relu(Z):\n",
        "    A=np.maximum(0,z)\n",
        "    cache=Z           \n",
        "    assert(A.shape == Z.shape)\n",
        "    return A,cache\n",
        "def sigmoid(Z):\n",
        "    A=1/(1+np.exp(-Z))\n",
        "    cache=Z\n",
        "    return A,cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT3j4cazOBRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation(A_prev,W,b,activation):\n",
        "    if activation==\"sigmoid\":\n",
        "        Z,linear_cache=forward_Z(A_prev,W,b)\n",
        "        A,activation_cache=sigmoid(Z)\n",
        "    elif activation==\"relu\":\n",
        "        Z,linear_cache=forward_Z(A_prev,W,b)\n",
        "        A,activation_cache=relu(Z)\n",
        "    cache=(linear_cache,activation_cache)\n",
        "    return A,cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRVfh5mOf19f",
        "colab_type": "text"
      },
      "source": [
        "For this neural network i will repeating relu for L-1 times and as a last step one sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8MbCOzGfwnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(X,parameters):\n",
        "    #Do [forward_z>ReLu(z)] for L-1 times and then [forward_z>sigmoid(z)] for 1 time. \n",
        "    #here X is the data. It's numpy array and its shape is(input data size,number of examples)    \n",
        "    caches=[]\n",
        "    A=X #here initial A is the X for each loop it will change. \n",
        "    L=len(parameters)//2  #consists of w_i and b_i so its length is 2*L\n",
        "    \n",
        "    #Relu for L-1 times:\n",
        "    for i in range(1,L):\n",
        "        A_prev=A\n",
        "        W=parameters[\"W\"+str(i)]\n",
        "        b=parameters[\"b\"+str(i)]\n",
        "        #calculate activation\n",
        "        A,cache=activation(A_prev,W,b,activation=\"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    #sigmoid for one time\n",
        "    W=parameters[\"W\"+str(L)]\n",
        "    b=parameters[\"b\"+str(L)]\n",
        "    AL,cache=activation(A,W,b,activation=\"sigmoid\") #here AL denotes a^{L}\n",
        "    caches.append(cache)\n",
        "    return AL, caches\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8zzpYKl-BJ9",
        "colab_type": "text"
      },
      "source": [
        "# 3.Cost Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIG4SbjwJoYt",
        "colab_type": "text"
      },
      "source": [
        "Next i will calculate the cost function: \n",
        "$$Cost=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(A^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- A^{[L](i)}\\right))$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc2IJT3mJ5yX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost(AL,Y):\n",
        "    #Here AL is the result of the sigmoid function(last step of the forward propagation)\n",
        "    #and Y is the true label vector.\n",
        "    m=Y.shape[1]\n",
        "    cost=-1/m*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
        "    cost=np.squeeze(cost) #if cost returns [0.1] this will turn it into 0.1\n",
        "    assert(cost.shape==())\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS29LPBNmYl4",
        "colab_type": "text"
      },
      "source": [
        "# 4.Back Propagation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fUcxRMNUQus",
        "colab_type": "text"
      },
      "source": [
        "In the 5th step i will update the parameters with the following formula:\n",
        "$$W=W-\\alpha dW$$\n",
        "$$b=b-\\alpha db$$ Where $\\alpha$ is the pre-determined learning rate and $dW$ and $db$ are the short notations for the following:\n",
        "\n",
        " $$dW^{[i]}=\\frac{\\partial J}{\\partial W^{[i]}}$$\n",
        "$$db^{[i]}=\\frac{\\partial J}{\\partial b^{[i]}}$$\n",
        "Here we will need to use chain rule in order to find above equations. The full chain is the following:\n",
        "$$\\frac{\\partial L}{\\partial w_1}=\\frac{\\partial L}{\\partial A^{[L]}}\\frac{\\partial A^{[L]}}{\\partial z}\\frac{\\partial z}{\\partial w_{1}}$$\n",
        "but i will assume $\\frac{\\partial L}{\\partial z}$ is given. So we will instead calculate: \n",
        "$$\\frac{\\partial L}{\\partial w_1}=\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial w_1}$$\n",
        "But we need $\\frac{\\partial J}{\\partial x}$ so we will sum over m examples and divide by m. Remember Z:\n",
        "\n",
        "$$Z^{[i]} = W^{[i]} A^{[i-1]} + b^{[i]}$$ So $dW^{[i]}$ becomes\n",
        "$$dW^{[i]}=\\frac{1}{m}{dZ^{[i]}*A^{[i-1]}.T}$$\n",
        "$$dA^{[i-1]}=W^{[i]}.T*dZ$$\n",
        "and $db^{[i]}$ becomes\n",
        "$$db^{[i]}=\\frac{1}{m}\\sum_{i = 1}^{m}{dZ^{[i]}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl_LvHBu839O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ,cache):\n",
        "    A_prev, W,b =cache\n",
        "    m.A_prev.shape[1]\n",
        "    dW=1/m*np.dot(dZ,A_prev.T)\n",
        "    db=1/m*np.sum(dZ,axis=1,keepdims=True)\n",
        "    dA_prev=np.dot(W.T,dZ)\n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "\n",
        "    \n",
        "    return dA_prev,dW,db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MayE2EGB98jn",
        "colab_type": "text"
      },
      "source": [
        "Now finding dZ:\n",
        "$$A=g(z)$$\n",
        "$$\\frac{dL}{dA}=\\frac{dL}{dZ}*g^{\\prime}(z)$$\n",
        "$$dZ=dA*g^{\\prime}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwL9LxE2LrTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_backward(dA,cache):\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) \n",
        "    #For Z>0 A(Z)=Z So derivative is =1\n",
        "    #For Z<0 A(Z)=0 Since this one is constant derivative  is =0\n",
        "    dZ[Z <= 0] = 0  #Z<=0 part just sets every element as True or False and the whole line sets True\n",
        "    #values to =0. We need to specify 0 terms\n",
        "    #For Z>0 part we won't implement anything since we have already copied dA.\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA,cache):\n",
        "    Z = cache\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA*s*(1-s)\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g92u30NQRG-l",
        "colab_type": "text"
      },
      "source": [
        "Combining the previous 2 cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svTM18ZNV3nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation_backward(dA,cache,activation):\n",
        "    linear_cache,activation_cache=cache\n",
        "    if activation==\"relu\":\n",
        "        dZ=relu_backward(dA,activation_cache)\n",
        "        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n",
        "    \n",
        "    elif activation==\"sigmoid\":\n",
        "        dZ=sigmoid_backward(dA,activation_cache)\n",
        "        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n",
        "    return dA_prev,dW,db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_c2MoQpcad0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_backward(AL,Y,caches):\n",
        "    grads={} #dA1=.. dA2=..\n",
        "    L=len(caches)\n",
        "    m=AL.shape[1]\n",
        "    Y=Y.reshape(AL.shape) # here Y is the true label vector. \n",
        "    dAL=-np.divide(Y/AL)-np.divide(1-Y,1-AL)\n",
        "    current_cache=caches[L-1]\n",
        "    #Lth layer(last step) sigmoid function\n",
        "    grads[\"dA\"+str(L-1)],grads[\"dW\"+str(L)],grads[\"db\"+str(L)]=activation_backward(dAL,current_cache,activation=\"sigmoid\")\n",
        "\n",
        "    #for (L-1)th layer to first layer (all Relu)\n",
        "    for i in reversed(range(L-1)):\n",
        "        current_cache=caches[i]\n",
        "        dA_prev_temp, dW_temp, db_temp = activation_backward(grads[\"dA\"+str(l+1)],current_cache,\"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIeMXriVE_oz",
        "colab_type": "text"
      },
      "source": [
        "# 5.Updating the parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjgIU5qWQxoB",
        "colab_type": "text"
      },
      "source": [
        "As a last step parameters will be updated: \n",
        "$$W=W-\\alpha dW$$\n",
        "$$b=b-\\alpha db$$ Where $\\alpha$ is the pre-determined learning rate and $dW$ and $db$ are the short notations for the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iGmjnbHFESv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L=len(parameters)//2\n",
        "    for i in range(L):\n",
        "        parameters[\"W\" + str(i+1)] = parameters[\"W\" + str(i+1)]-learning_rate*grads[\"dW\" + str(i+1)]\n",
        "        parameters[\"b\" + str(i+1)] = parameters[\"b\" + str(i+1)]-learning_rate*grads[\"db\" + str(i+1)]\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}